import { ModelProfile } from '../types';

export const modelCatalog: ModelProfile[] = [
  {
    id: 'gpt-4o-mini',
    provider: 'openai',
    displayName: 'GPT-4o mini',
    supportsVision: true,
    supportsDocs: true,
    supportsOcrCleanup: true,
    avgLatencyMs: 900,
    qualityScore: 8.3,
    costPer1kTokensUsd: 0.0006,
    freeTierLikely: true,
  },
  {
    id: 'claude-3-haiku',
    provider: 'anthropic',
    displayName: 'Claude 3 Haiku',
    supportsVision: true,
    supportsDocs: true,
    supportsOcrCleanup: true,
    avgLatencyMs: 1100,
    qualityScore: 8.6,
    costPer1kTokensUsd: 0.0008,
    freeTierLikely: false,
  },
  {
    id: 'deepseek-chat',
    provider: 'deepseek',
    displayName: 'DeepSeek Chat',
    supportsVision: false,
    supportsDocs: true,
    supportsOcrCleanup: true,
    avgLatencyMs: 950,
    qualityScore: 8.1,
    costPer1kTokensUsd: 0.0002,
    freeTierLikely: true,
  },
  {
    id: 'qwen2.5-72b',
    provider: 'qwen',
    displayName: 'Qwen 2.5 72B',
    supportsVision: false,
    supportsDocs: true,
    supportsOcrCleanup: true,
    avgLatencyMs: 1200,
    qualityScore: 8.5,
    costPer1kTokensUsd: 0.0004,
    freeTierLikely: true,
  },
  {
    id: 'mistral-small',
    provider: 'mistral',
    displayName: 'Mistral Small',
    supportsVision: true,
    supportsDocs: true,
    supportsOcrCleanup: true,
    avgLatencyMs: 1000,
    qualityScore: 8.0,
    costPer1kTokensUsd: 0.0005,
    freeTierLikely: true,
  },
  {
    id: 'llama3.1-local',
    provider: 'llama-local',
    displayName: 'Llama 3.1 (Ollama)',
    supportsVision: false,
    supportsDocs: true,
    supportsOcrCleanup: true,
    avgLatencyMs: 1600,
    qualityScore: 7.2,
    costPer1kTokensUsd: 0,
    freeTierLikely: true,
  },
];
